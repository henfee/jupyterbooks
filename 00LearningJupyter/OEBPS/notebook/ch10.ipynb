{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   "<html xmlns=\"http://www.w3.org/1999/xhtml\"><head><title>Chapter&#xA0;10.&#xA0;Jupyter and Big Data</title><link rel=\"stylesheet\" href=\"epub.css\" type=\"text/css\"/><meta name=\"generator\" content=\"DocBook XSL Stylesheets V1.75.2\"/></head><body id=\"page\"><div class=\"chapter\" title=\"Chapter&#xA0;10.&#xA0;Jupyter and Big Data\"><div class=\"titlepage\"><div><div><h1 class=\"title\"><a id=\"ch10\"/>Chapter&#xA0;10.&#xA0;Jupyter and Big Data</h1></div></div></div><p>Big data is the topic on everyone's mind. I thought it would be good to see what can be done with big data in Jupyter. An up-and-coming language for dealing with large datasets is Spark. Spark is an open source big data processing framework. Spark can run over Hadoop, in the cloud, or standalone. We can use Spark coding in Jupyter much like the other languages we have seen.</p><p>In this chapter, we will cover the following topics:</p><div class=\"itemizedlist\"><ul class=\"itemizedlist\"><li class=\"listitem\" style=\"list-style-type: disc\">Installing Spark for use in Jupyter</li><li class=\"listitem\" style=\"list-style-type: disc\">Using Spark's features</li></ul></div><div class=\"section\" title=\"Apache Spark\"><div class=\"titlepage\"><div><div><h1 class=\"title\"><a id=\"ch10lvl1sec78\"/>Apache Spark</h1></div></div></div><p>One of the tools we will be using is Apache Spark. Spark is an open source toolset for cluster computing. While we will not be using a cluster, the typical usage for Spark is a larger set of machines or cluster that operate in parallel to analyze a big data set. An installation guide is available at <a class=\"ulink\" href=\"https://www.dataquest.io/blog/pyspark-installation-guide\">https://www.dataquest.io/blog/pyspark-installation-guide</a>.&#xA0;In particular, you will need to add two settings to your bash profile: <code class=\"literal\">SPARK_HOME</code> and <code class=\"literal\">PYSPARK_SUBMIT_ARGS</code>. <code class=\"literal\">SPARK_HOME</code> is the directory where the software is installed. <code class=\"literal\">PYSPARK_SUBMIT_ARGS</code> sets the number of cores to use in the local cluster.</p><div class=\"section\" title=\"Mac installation\"><div class=\"titlepage\"><div><div><h2 class=\"title\"><a id=\"ch10lvl2sec56\"/>Mac installation</h2></div></div></div><p>To install, we download the latest TGZ file from the Spark download page at <a class=\"ulink\" href=\"https://spark.apache.org/downloads.html\">https://spark.apache.org/downloads.html</a>, unpack the TGZ file, and move the unpacked directory to our Applications folder.</p><p>Spark relies on Scala's availability. We installed Scala in \n<a class=\"link\" href=\"ch07.html\" title=\"Chapter&#xA0;7.&#xA0;Sharing and Converting Jupyter Notebooks\">Chapter 7</a>,&#xA0;\n<span class=\"emphasis\"><em>Sharing and Converting Jupyter Notebooks</em></span>.</p><p>Open a command-line window to the Spark directory and run this command:</p><pre class=\"programlisting\">\n<span class=\"strong\"><strong>brew install sbt</strong></span>\n</pre><p>This may take a while.</p><p>Now set the configuration for Spark (for Mac) in your <code class=\"literal\">.bash_profile</code> file:</p><pre class=\"programlisting\">\n<span class=\"strong\"><strong># location of spark code</strong></span>\n<span class=\"strong\"><strong>export SPARK_HOME=\"/Applications/spark-2.0.0-bin-hadoop2.7\"</strong></span>\n<span class=\"strong\"><strong># machine to run on</strong></span>\n<span class=\"strong\"><strong>export SPARK_MASTER_IP=127.0.0.1</strong></span>\n<span class=\"strong\"><strong>export SPARK_LOCAL_IP=127.0.0.1</strong></span>\n<span class=\"strong\"><strong># python location</strong></span>\n<span class=\"strong\"><strong>export PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH</strong></span>\n<span class=\"strong\"><strong>export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.1-src.zip:$PYTHONPATH</strong></span>\n</pre><p>Note that, the paths used will correspond to your installation</p><p>You should now be able to run this command (from inside your Spark directory), successfully opening a command-line window in Spark:</p><pre class=\"programlisting\">\n<span class=\"strong\"><strong>bin/pyspark</strong></span>\n</pre><p>It looks something like this (depending on the version):</p><pre class=\"programlisting\">\n<span class=\"strong\"><strong>Welcome to</strong></span>\n<span class=\"strong\"><strong>      ____              __</strong></span>\n<span class=\"strong\"><strong>     / __/__  ___ _____/ /__</strong></span>\n<span class=\"strong\"><strong>    _\\ \\/ _ \\/ _ `/ __/  '_/</strong></span>\n<span class=\"strong\"><strong>   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.0</strong></span>\n<span class=\"strong\"><strong>      /_/</strong></span>\n<span class=\"strong\"><strong>Using Python version 2.7.12 (default, Jul  2 2016 17:43:17)</strong></span>\n<span class=\"strong\"><strong>SparkSession available as 'spark'.</strong></span>\n<span class=\"strong\"><strong>&gt;&gt;&gt;</strong></span>\n</pre><p>You can enter <code class=\"literal\">quit()</code> to exit.</p><p>Now, when we run our notebook, when using a Python kernel, we can access Spark.</p></div><div class=\"section\" title=\"Windows installation\"><div class=\"titlepage\"><div><div><h2 class=\"title\"><a id=\"ch10lvl2sec57\"/>Windows installation</h2></div></div></div><p>We have already installed Python as part of the Jupyter installation much earlier in this book. We need to download and install the latest Spark version from <a class=\"ulink\" href=\"http://spark.apache.org/downloads.html\">http://spark.apache.org/downloads.html</a>. Unpack the TGZ file and move the resulting directory to the <code class=\"literal\">C:\\spark directory</code>.</p><p>You will need to have <code class=\"literal\">winutils.exe</code> available as well (this seems to be a problem with the Hadoop installation, but it may get fixed at some time). Download the file from <a class=\"ulink\" href=\"http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe\">http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe</a> and install at <code class=\"literal\">C:\\winutils\\bin</code>.</p><p>Now need to set up environment variables for all of these:</p><pre class=\"programlisting\">\n<span class=\"strong\"><strong>HADOOP_HOME=C:\\winutils</strong></span>\n<span class=\"strong\"><strong>SPARK_HOME=C:\\spark</strong></span>\n<span class=\"strong\"><strong>PYSPARK_DRIVER_PYTHON=ipython</strong></span>\n<span class=\"strong\"><strong>PYSPARK_DRIVER_PYTHON_OPTS=notebook</strong></span>\n</pre><p>You can start Jupyter using the <code class=\"literal\">pyspark</code> command. You should not notice anything different about your notebook.</p><div class=\"note\" title=\"Note\" style=\"\"><div class=\"inner\"><h3 class=\"title\"><a id=\"note32\"/>Note</h3><p>We are using the Python script to invoke Spark functionality, so the language format needs to conform to Python.</p></div></div></div></div></div></body></html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
